{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Token Interactions\n",
    "\n",
    "This notebook dives into virtual token interactions through the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.transformer import Transformer, Config\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from einops import *\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "color = dict(color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "\n",
    "name = \"tdooms/TinyStories-1-512\"\n",
    "config = Config.from_pretrained(name)\n",
    "model = Transformer.from_pretrained(name, config=config).cuda()\n",
    "\n",
    "model.center_unembed().fold_norms()\n",
    "vocab = model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing MLPs in our single-layer bilinear models, we can construct all kinds of tensors describing interactions in the latent (residual) space. We can then project these through the embedding or unembedding to get the actual tokens that are being manipulated through the network. This operation is a simplification as it ignores the attention mechanism. Therefore, we call this the \"direct embedding\" because it assumes only taking the direct path.\n",
    "\n",
    "When including the attention mechanism, projecting out the latent space isn't as straightforward. In essence, the latent space is now a sum of the attention outputs and of the direct path. \n",
    "Formulaically, we have.\n",
    "\n",
    "$$residual^{mid} = \\sum_i (\\lambda_i OV_{i}E t_v) + E t_d$$\n",
    "\n",
    "Where $\\lambda_i = t_d QK_i t_i$. if we ignore $\\lambda$, we seen that the residual is simply a sum of paths. So, instead of only projecting out of $E$ as in the direct path, we can project out the other terms too. \n",
    "\n",
    "``TODO``: write some coherent story.\n",
    "\n",
    "We define virtual tokens as tokens that have passed through the OV circuit of an attention head. Our full embedding then becomes $\\text{cat}(E, OV_0 E, OV_1 E)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The augmented embedding, containing direct and indirect embeddings\n",
    "# This is actually a stack of embeddings, which is easier to work with\n",
    "# Note that the direct path is the first element\n",
    "e_full = torch.cat([model.w_e[None], model.ov[0] @ model.w_e[None]], dim=0)\n",
    "b = model.b[0]\n",
    "\n",
    "# We project b through the full embedding to get the interactions between virtual and direct tokens\n",
    "# We can't construct this full tensor though, therefore, we only do so for a single token\n",
    "token = \"girl\"\n",
    "idx = vocab[token]\n",
    "\n",
    "blocks = einsum(e_full, e_full, b, model.w_u[idx], \"b1 hid1 tok1, b2 hid2 tok2, out hid1 hid2, out -> b1 b2 tok1 tok2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take any norm of the specific interaction blocks of the heads to see how strong they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1_norms = blocks.mean((2, 3))\n",
    "l2_norms = torch.linalg.norm(blocks, dim=(2, 3))\n",
    "\n",
    "title = f\"Attention Head Interactions for \\\"{token}\\\"\"\n",
    "px.imshow(l2_norms.cpu(), **color, title=title).update_layout(title_x=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this gives is a map from {token -> attention interaction}. However, ideally, we'd like the inverse map. This would allow us to see which tokens maximally activate certain attention head interactions.\n",
    "\n",
    "Luckily, creating that isn't actually very difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_summed = e_full.mean(-1)\n",
    "means = einsum(e_summed, e_summed, b, model.w_u, \"b1 hid1, b2 hid2, res hid1 hid2, out res -> out b1 b2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(means[vocab[\"girl\"]].cpu(), **color, title=title).update_layout(title_x=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some observations. \n",
    "- In the single layer 256 model (which has 4 attention heads), most stuff seems to happen in the direct-direct path.\n",
    "- The single layer 512 model (8 heads) has the strongest interactions through attention head 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = einsum(model.w_e, model.w_e, model.qk[0], \"hid1 in1, hid2 in2, ... hid1 hid2 -> ...\")\n",
    "px.bar(y=traces.cpu(), labels=dict(x=\"trace\", y=\"head\"), title=\"trace of QK circuits\").update_layout(title_x=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find the output tokens which are most positively or negatively activated for each pair of heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=list(range(20)))\n",
    "\n",
    "for i, j in itertools.combinations_with_replacement(range(config.n_head), 2):\n",
    "    df[f\"{i}-{j}\"] = vocab.tokenize(means[:, i, j].abs().topk(20).indices)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of structure is quite limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(means.mean(0).cpu(), **color, title=\"Mean Interaction Strength\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
