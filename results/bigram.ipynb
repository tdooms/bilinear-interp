{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams in Bilinear Transformers\n",
    "\n",
    "Bilinear transformers are great because they are even more linear in nature than the original architecture. This allows us to perform standardized analysis on each component separately (or even together). This notebook in particular focusses on extracting 2-grams from the weights. This notebook is meant as an introduction to the capabilities of bilinear layers and shouldn't be used to draw rigorous conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from shared.transformer import Transformer, Config\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import pandas as pd\n",
    "from einops import *\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "color = dict(color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "name = \"tdooms/TinyStories-4-256\" \n",
    "\n",
    "config = Config.from_pretrained(name)\n",
    "model = Transformer.from_pretrained(name, config=config).cuda()\n",
    "\n",
    "model.center_unembed().fold_norms()\n",
    "vocab = model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 and 2 layer transformers have slightly different behavior. The 1-layer transformer has a slightly more diverse MLP layer (because it kinda has to). Results shown in this notebook hold for both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Path\n",
    "\n",
    "Let's start with the obvious way to look at 2-grams, the direct embedding-unembedding path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = (model.w_u @ model.w_e).detach().cpu()\n",
    "assert direct.shape == (len(vocab), len(vocab))\n",
    "\n",
    "vocab.get_max_activations(direct.T, [\"input\", \"output\"], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm a bit surprised that this doesn't make a lot of sense. I would expect some structure.\n",
    "\n",
    "TODO: why doesn't this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP path\n",
    "\n",
    "Now, onto the good stuff, the MLP. In a normal neural network, we can't study the MLP with SVD or any linear technique. However, bilinear layers actually allow us to do so. In this section, we will limit ourselves to the direct MLP path, aka embedding -> MLP -> unembedding. To our knowledge, this hasn't been done before. To study the direct path, we can take the diagonal over the last two dimensions of the B tensor. I won't go into the math here for brevity, trust me bro. \n",
    "\n",
    "Before looking at the eigenvalues, let's look at the highest activations in general, this will result in a map of input -> output, meaning that we get the pairs of which the model is most sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = model.ube.diagonal(residual=True).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a helper function ``get_max_activations`` This returns a data frame of indices and values of the max values in the provided tensor. The indices are automatically converted to tokens. Let's look at the top 1000 connections in the first MLP layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = vocab.get_max_activations(diag[0].T, [\"input\", \"output\"], k=1_000, largest=True)\n",
    "df0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so it's obvious that the most first layer just connects the obvious bi-grams of words that didn't quite get included in the tokenizer. Let's quantify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df0[\"output\"].str.startswith(\"##\").cumsum(), title=\"cumulative ## tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first layer seems to only be bothered with learning these kinds of connections. Let's look at the second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = vocab.get_max_activations(diag[1].T, [\"input\", \"output\"], k=1_000, largest=True)\n",
    "df1.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That makes way less sense. Let's try to look if there's a pattern on the tokens it activates strongly on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mean = diag[1].mean(1).view(64, 64)\n",
    "output_mean = diag[1].mean(0).view(64, 64)\n",
    "\n",
    "fig = px.imshow(torch.stack((input_mean, output_mean)), facet_col=0, **color)\n",
    "fig.layout.annotations[0].update(text=\"input\")\n",
    "fig.layout.annotations[1].update(text=\"output\")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except from the fact that the inputs seem to mostly looking positively at a handful of tokens, nothing discernable, we'll leave this inspection for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preceding and Following tokens\n",
    "\n",
    "Given this diagonal matrix, we can also analyze which words are most important indicators for the next word or the other way around.\n",
    "For instance, we can ask:\n",
    "- *\"what tokens are most important for the model to decide to predict the token 'game'\"* (preceding token).\n",
    "- *\"what tokens does the token 'game' infer most\"* (following token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"girl\"\n",
    "idx = vocab[token]\n",
    "\n",
    "preceding = vocab.tokenize(torch.topk(diag[0, idx], k=10).indices)\n",
    "following = vocab.tokenize(torch.topk(diag[0, :, idx], k=10).indices)\n",
    "\n",
    "pd.DataFrame(dict(preceding=preceding, self=[token]*10, following=following))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left and right are not related, this is simply a concise visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Articles\n",
    "\n",
    "Something interesting to look at is if the model has learned to use correct articles. Let's study this a bit more in-depth.\n",
    "We can do this quite simply by taking the weights for both for all subsequent tokens and plotting them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_non_words = torch.tensor([vocab.inv[idx][0].isalpha() for idx in range(len(vocab))])\n",
    "\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "\n",
    "token = vocab.tokenize(torch.arange(len(vocab)))\n",
    "df = pd.DataFrame(dict(x=diag[0, :, vocab[\"a\"]].cpu(), y=diag[0, :, vocab[\"an\"]].cpu(), token=token))\n",
    "df = df[df.token.str[0].str.isalpha()]\n",
    "df[\"guess\"] = df.token.str[0].isin(vowels)\n",
    "\n",
    "px.scatter(df, x=\"x\", y=\"y\", hover_name=\"token\", color=\"guess\", labels=dict(x=\"a\", y=\"an\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result isn't as clean as I'd hoped but it seems that the model simply generally has a strong bias towards picking 'a' which is sensible. If you hover over most tokens, it's clear why it's \"unsure\" about some of them, a proper filtering of verbs and such will probably improve the separation. Alos, I'd assume this becomes more clear as models improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Interactions\n",
    "Until now, we've only looked at the direct path. This is fine, but the MLP encodes so much more information than (input output)-pairs. Specifically, it actually encodes (input, input, output)-triplets, being one of the reasons for its effectiveness.\n",
    "\n",
    "So, in essence, until now, we've just looked at token interactions with itself. This reduces the UBE tensor to a matrix, which we can study. Now, we will perform another reduction, by just taking the first dimension of the UBE tensor, which means that we will get the input-input interactions for a certain token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = vocab[\"boy\"]\n",
    "inter = model.ube.interaction(idx, residual=True).cpu()[0]\n",
    "\n",
    "topk = torch.topk(inter.tril().flatten(), k=25)\n",
    "input1, input2 = torch.unravel_index(topk.indices, inter.size())\n",
    "pd.DataFrame(dict(input1=vocab.tokenize(input1), input2=vocab.tokenize(input2), value=topk.values.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will mostly become useful once we introduce some additional inspection techniques in later attention layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
