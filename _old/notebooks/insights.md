# Tinystories Insights

This is a small collection of random insights and results.

---

- Without attention, the model can only achieve a loss of ~3.5. Given that even the small models achieve losses way lower than that means that attention is **really** important, even in a 1 layer model.
- Actually, leaving the MLP is the same as removing it when there is no attention.