{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilinear Toy Models of Decomposition\n",
    "**By Michael Pearce and Thomas Dooms**\n",
    "\n",
    "Activation functions are a vital component in deep networks to learn complex functions. The sole purpose of an activation function is to introduce non-linearity into the network to prevent matrix collapse. It is often argued that the simplest non-linearity is piecewise linearity, which is exactly what the most popular activation function, the ReLU, does.\n",
    "\n",
    "However, in terms of interpretability, ReLUs are very difficult to study. Intuitively, the issue is that it is only possible to know the output of a ReLU by passing in the input, not solely from the weights. While it is possible to discern structure and circuits in these weights, this is often done by means of sampling the input or using gradient based techniques to generate visualizations which may not reflect the full complexity of the models. This makes it impossible to make strong guarantees on which outputs models will be able to produce. This has been observed in the form of adversarial examples, in which a slight perturbation is applied to the input to confuse the model into making bogus predictions.\n",
    "\n",
    "This undesirable property of the ReLU has led to MLPs and similar structures being famously hard to interpret. In this document, we make the design decision to replace ReLUs with the more interpretable bilinear layer. We provide an introduction to these layers and provide an overview of our current efforts in interpreting simple models using them.\n",
    "\n",
    "#### Introduction to Bilinear Maps\n",
    "\n",
    "What follows is a very intuitive explanation of bilinear maps, if you're already familiar with the concept, you're free to skip this part.\n",
    "\n",
    "<!-- To understand bilinear maps, let's start with linear maps. Simply put, a matrix is a linear map, it maps a vector space to another space using two properties: vector addition and scalar multiplication.\n",
    "Let's define our linear map as $f$ The first property states that $f(v+w) = f(v) + f(w)$. The second states -->\n",
    "\n",
    "Simply put, a matrix is the most general form of a linear map. It satisfies all linear properties that we usually expect. One can think of a matrix as a function that takes a vector as input and returns a new vector as output. In contrast, a bilinear map takes in two vectors and spits returns a vector according to all the same linear properties. These are some intuition-pumps for this concept.\n",
    "\n",
    "- We can use one of the inputs to compute a matrix. So, in a sense a bilinear operation is like a function that takes in a vector and returns a matrix. Then this matrix can be used with the second input to compute the output.\n",
    "- TODO: more intuition\n",
    "\n",
    "Intuitively, the mathematical properties of a bilinear map are comparable to scalar multiplication. Multiplication takes in two numbers and returns another and each of the inputs is linear if we freeze the other.\n",
    "\n",
    "#### Bilinear Maps in Neural Nets\n",
    "\n",
    "A normal layer in a neural network is structured as follows: $h_{i+1} = ReLU(W_i h_i + b_i)$. We will study the following structure: $h_{i+1} = (W_i h_i + b^w_i) \\odot (V_i h_i + b^v_i)$ where $W_i$ and $V_i$ are matrices of the same dimensionality and $b^w_i$ and $b^v_i$ are biases. The $\\odot$ denotes an element-wise product. The important part is that this operation is non-linear with regards to the input; it is not possible to define a matrix that encodes this operation. It is however possible to define a tensor of rank 3 that does. While interpreting tensors is more difficult matrices, we now actually have a geometric object that describes the whole layer *can* be studied on its own.\n",
    "\n",
    "We can construct this tensor using the the following product of $V$ and $W$.\n",
    "\n",
    "```python\n",
    "B = einsum(V, W, \"output input1, output input2 -> output input1 input2\")\n",
    "```\n",
    "\n",
    "#### Motivation & goals\n",
    "The main goal of this project is to fully understand (small) networks that only use bilinear layers. We do this by studying their weights on small toy models. This study allows us to design interpretability techniques in a controlled environment to evaluate which works best.\n",
    "\n",
    " -- TODO\n",
    "\n",
    "\n",
    "#### Toy Models\n",
    "In this write-up, we will focus on small models consisting of a linear encoder, a bilinear layer and a decoder. Specifically, we study the following architecture:\n",
    "\n",
    "Or mathematically:\n",
    "$$h = Ex$$\n",
    "$$h' = (W+b^w) h \\odot (V+b^v) h$$\n",
    "$$y = Dh'$$\n",
    "\n",
    "This can also be written as a single function.\n",
    "\n",
    "$$y = D(WEx \\odot VEx)$$\n",
    "\n",
    "#### Constructing Tensors\n",
    "Before delving into the next parts, it's important to grasp which objects we will study and how they are constructed. There are 3 main concepts to grasp:\n",
    "- Feature interactions\n",
    "- Including biases\n",
    "- SVD decomposition\n",
    "\n",
    "\n",
    "\n",
    "For instance, we stated that a bilinear layer can be represented as a rank 3 tensor that is computed by taking a tensor product between $W$ and $V$. However, this does not include any biases, which we use in our architecture.\n",
    "\n",
    "> Aside: Biases play an unsung role in neural networks, they allow the network to learn values that are independent of the input. Recently, these biases are mostly pushed towards the normalization layers but their role remains equally important. Within bilinear layers, biases become even more important. Concretely, without biases it is impossible to learn the identity matrix or any linear operation. TODO: I can add some more \n",
    "\n",
    "TODO: I should probably add some math here\n",
    "\n",
    "Studying bilinear layers with biases requires a small trick; we incorporate them into the matrix weights $W$ and $V$. \n",
    "\n",
    "Mathematically we construct:\n",
    "- $x' = [\\vec{x} \\: 1]$\n",
    "- $W' = [\\vec{W_i} \\: b^w_i]$\n",
    "- $V' = [\\vec{V_i} \\: b^v_i]$\n",
    "\n",
    "Intuitively, if we append a constant factor the input vector and an extra dimension to both $W$ and $V$ matrices. This is equivalent to using a bias.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### General Layout of This Notebook\n",
    "We aim to strike a balance between providing all code but remaining readable. Therefore, we import some generic code such as visualization functions, models and utility functions from [our github repo](https://github.com/tdooms/bilinear-interp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install einops\n",
    "!pip install jaxtyping\n",
    "!git clone https://github.com/tdooms/bilinear-interp.git\n",
    "\n",
    "%cd bilinear-interp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Superposition\n",
    "Studying superposition is a solid start for what we wish to achieve. This is mainly due to the fact that the setup is simple and the outcomes are exactly known (for normal networks). The setup we use for eliciting superposition is similar to [the original superposition paper](https://transformer-circuits.pub/2022/toy_model/index.html). specifically, we use the following:\n",
    "\n",
    "- The encoder projects down into fewer dimensions than the input.\n",
    "- We do not use the decoder (aka we use an identity operation).\n",
    "- The loss is the mean squared error between the prediction and the input.\n",
    "- We study different sparsities of the input.\n",
    "\n",
    "Let's define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
